{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f194f62",
   "metadata": {},
   "source": [
    "## Step 5: Save the Model and Preprocessor\n",
    "- Save the trained XGBoost model and the PCA object using pickle.\n",
    "- These files will be used by our Flask API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a63f254",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate the Model\n",
    "- Make predictions on the test set.\n",
    "- Check the accuracy and other metrics to confirm performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64af64af",
   "metadata": {},
   "source": [
    "## Step 3: Train the XGBoost Model\n",
    "- [cite_start]Initialize and train the XGBClassifier on the pre-processed data[cite: 167].\n",
    "- [cite_start]The paper achieved 98.2% accuracy with this model[cite: 16]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bec595a",
   "metadata": {},
   "source": [
    "## Step 2: Pre-process the Data\n",
    "- Separate features (X) and the target variable (y).\n",
    "- [cite_start]Split the data into training (80%) and testing (20%) sets[cite: 133].\n",
    "- [cite_start]Apply SMOTE to the training data to handle the imbalanced classes[cite: 40, 152].\n",
    "- [cite_start]Apply PCA for dimensionality reduction[cite: 43, 159]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b01d0f",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Load Data\n",
    "- Import pandas, SMOTE, PCA, and XGBoost.\n",
    "- Load the fraud detection dataset using pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dd4f51",
   "metadata": {},
   "source": [
    "# UPI Fraud Detection Model Training\n",
    "\n",
    "### Project Goal: To build and train an XGBoost model to detect fraudulent transactions based on the provided research paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8b2a66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Script Starting ---\n",
      "âœ… Step 1: Libraries imported successfully. Using NumPy version: 1.26.4\n",
      "âœ… Step 2: Dataset '../creditcard.csv' loaded successfully.\n",
      "\n",
      "--- Starting Data Pre-processing ---\n",
      "    > Using numeric columns for training: ['Unnamed: 0', 'cc_num', 'amt', 'zip', 'lat', 'long', 'city_pop', 'unix_time', 'merch_lat', 'merch_long']\n",
      "    > Data split into training and testing sets.\n",
      "    > Data scaled using StandardScaler.\n",
      "    > Applying SMOTE to balance the data... (This may take a minute)\n",
      "    > SMOTE applied successfully.\n",
      "    > PCA applied for feature extraction.\n",
      "âœ… Step 3: Data pre-processing complete.\n",
      "\n",
      "--- Starting Model Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhanu\\anaconda3\\envs\\final_project_env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [18:19:13] WARNING: D:\\bld\\xgboost-split_1758007502304\\work\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Step 4: XGBoost model training complete.\n",
      "\n",
      "--- Starting Model Evaluation ---\n",
      "âœ… Step 5: Model evaluation complete. Final Accuracy: 97.89%\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99    110715\n",
      "           1       0.13      0.81      0.23       429\n",
      "\n",
      "    accuracy                           0.98    111144\n",
      "   macro avg       0.57      0.89      0.61    111144\n",
      "weighted avg       1.00      0.98      0.99    111144\n",
      "\n",
      "\n",
      "--- Saving Model and Preprocessor ---\n",
      "    > Model saved to: ../model\\xgboost_model.pkl\n",
      "    > Preprocessor saved to: ../model\\preprocessor.pkl\n",
      "âœ… Step 6: Model and preprocessor saved successfully.\n",
      "\n",
      "ðŸš€ ALL DONE! You can now run your Flask application. ðŸš€\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# FINAL, COMPLETE MODEL TRAINING SCRIPT\n",
    "# ===================================================================\n",
    "\n",
    "# !!! IMPORTANT !!!\n",
    "# The fraud column name has been corrected below.\n",
    "# -------------------------------------------------------------------\n",
    "FRAUD_COLUMN_NAME = 'is_fraud'  # <-- THIS LINE IS NOW CORRECT\n",
    "\n",
    "# ===================================================================\n",
    "# (Do not change anything below this line)\n",
    "# ===================================================================\n",
    "\n",
    "print(\"--- Script Starting ---\")\n",
    "\n",
    "# Step 1: Import All Necessary Libraries\n",
    "try:\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    import pickle\n",
    "    import os\n",
    "    import numpy as np\n",
    "    print(f\"âœ… Step 1: Libraries imported successfully. Using NumPy version: {np.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ ERROR: A required library is not installed. Details: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 2: Load the Dataset\n",
    "try:\n",
    "    dataset_path = '../creditcard.csv'\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    print(f\"âœ… Step 2: Dataset '{dataset_path}' loaded successfully.\")\n",
    "    if FRAUD_COLUMN_NAME not in df.columns:\n",
    "        print(f\"âŒ FATAL ERROR: The column '{FRAUD_COLUMN_NAME}' was not found in your dataset.\")\n",
    "        print(f\"    Available columns are: {df.columns.tolist()}\")\n",
    "        raise KeyError(f\"Column '{FRAUD_COLUMN_NAME}' not found.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ FATAL ERROR: 'creditcard.csv' not found in the 'ml_service' folder.\")\n",
    "    raise\n",
    "\n",
    "# Step 3: Pre-process the Data\n",
    "print(\"\\n--- Starting Data Pre-processing ---\")\n",
    "\n",
    "# Before we can train the model, we must handle non-numeric data.\n",
    "# We will select only the numeric columns for this model.\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "# Ensure the fraud column is not accidentally dropped if it's numeric\n",
    "if FRAUD_COLUMN_NAME in numeric_cols:\n",
    "     numeric_cols.remove(FRAUD_COLUMN_NAME)\n",
    "\n",
    "print(f\"    > Using numeric columns for training: {numeric_cols}\")\n",
    "\n",
    "# Separate features (X) and the target variable (y) using the variable you set\n",
    "X = df[numeric_cols]\n",
    "y = df[FRAUD_COLUMN_NAME]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(\"    > Data split into training and testing sets.\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"    > Data scaled using StandardScaler.\")\n",
    "\n",
    "print(\"    > Applying SMOTE to balance the data... (This may take a minute)\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "print(\"    > SMOTE applied successfully.\")\n",
    "\n",
    "pca = PCA(n_components=10) # Using 10 components as there are fewer numeric columns\n",
    "X_train_pca = pca.fit_transform(X_train_smote)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "print(\"    > PCA applied for feature extraction.\")\n",
    "print(\"âœ… Step 3: Data pre-processing complete.\")\n",
    "\n",
    "# Step 4: Train the XGBoost Model\n",
    "print(\"\\n--- Starting Model Training ---\")\n",
    "model = XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train_pca, y_train_smote)\n",
    "print(\"âœ… Step 4: XGBoost model training complete.\")\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "print(\"\\n--- Starting Model Evaluation ---\")\n",
    "y_pred = model.predict(X_test_pca)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"âœ… Step 5: Model evaluation complete. Final Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 6: Save the Model and Preprocessor\n",
    "print(\"\\n--- Saving Model and Preprocessor ---\")\n",
    "model_dir = '../model'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(model_dir, 'xgboost_model.pkl')\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print(f\"    > Model saved to: {model_path}\")\n",
    "\n",
    "preprocessor_path = os.path.join(model_dir, 'preprocessor.pkl')\n",
    "with open(preprocessor_path, 'wb') as f:\n",
    "    pickle.dump((scaler, pca), f)\n",
    "print(f\"    > Preprocessor saved to: {preprocessor_path}\")\n",
    "print(\"âœ… Step 6: Model and preprocessor saved successfully.\")\n",
    "\n",
    "print(\"\\nðŸš€ ALL DONE! You can now run your Flask application. ðŸš€\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "863738bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'trans_date_trans_time', 'cc_num', 'merchant', 'category',\n",
      "       'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip',\n",
      "       'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time',\n",
      "       'merch_lat', 'merch_long', 'is_fraud'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../creditcard.csv')\n",
    "print(df.columns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
